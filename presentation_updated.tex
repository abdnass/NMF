
\documentclass[aspectratio=169]{beamer}
% metadata
\title{Nonnegative Matrix Factorization}
\subtitle{Be Positive!}
\author{Abdelbast Nassiri \\ Maximilian Stollmayer \\ Manuel Wissiak}
\institute{University of Vienna}
\date{30.01.2023}

% \usetheme[titlestyle=plain, sectionstyle=style1, slidestyle=style1]{trigon}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{hyperref}
% \usepackage{setspace}
% \usepackage{xpatch}
% \usepackage{lipsum}
% \usepackage{tcolorbox}
% \usepackage{graphicx}
% \tcbuselibrary{fitting}
% \hypersetup{
%     colorlinks=true,
%     linkcolor=blue,
%     filecolor=magenta,      
%     urlcolor=cyan,
%     pdftitle={Overleaf Example},
%     pdfpagemode=FullScreen,
%     }
% 
% \RestyleAlgo{ruled}
% % \usepackage{algorithm}
% % \usepackage{algorithmic}
% % \usepackage{pseudocode}
% % \usepackage{float}
% \makeatletter
% \patchcmd\beamer@@tmpl@frametitle{\insertframetitle}{\insertsection\space-- \insertframetitle}{}{}
% \makeatother

% theme
\usetheme[titlestyle=plain, sectionstyle=style1, slidestyle=style1]{trigon}

% packages
\usepackage{multicol} % for two columns in algorithms section
    \setlength{\columnseprule}{1pt}
    \setlength{\columnsep}{1cm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{algorithm2e}
\usepackage{optidef}
\usepackage{epigraph}
\usepackage{array}


\begin{document}

% title ------------------------------------------------------------------------
\titleframe

% Manuel ----------------------------------------------------------------------
% https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8116d41cb547671bb9c38ffe47574e5cdf24fa7e
% https://ar5iv.labs.arxiv.org/html/1401.5226
% https://ar5iv.labs.arxiv.org/html/1703.00663
% https://ar5iv.labs.arxiv.org/html/0708.4149

\begin{frame}{NMF}
    \begin{block}{Problem}
        Given \(A \in \mathbb{R}^{m \times n}_+\) non-negative and \emph{rank} \(r \leq \min(m, n)\). \\
        Find \(W \in \mathbb{R}^{m \times r}_+\) and \(H \in \mathbb{R}^{r \times n}_+\) both non-negative s.t.: \\
        \[A \approx WH\]
    \end{block}
\end{frame}

\begin{frame}{Text Mining}
    \epigraph{"Words which are similar in meaning occur in similar contexts"} {\textit{CHATGPT}}
    picture \\
    % https://www.researchgate.net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a_fig1_312157184
    Visible Variables = Weights * Hidden Variables
    
\end{frame}

\begin{frame}{Image Processing}
    Given vectorized gray-levels \(X \in \mathbb{R}^{p \times n}_+\) of a facial image. \\
    Problem: Facial Feature Extraction
    \begin{center}
    \begin{tabular}{ c c l }
     \(X(:,j) \approx \) &  \(\sum_{k=1}^{r} W(:,k) \) & \(H(k,j)\) \\ 
     j-th facial image & facial features & importance of feature in j-th image 
    \end{tabular}
    \end{center}
     
\end{frame}

\begin{frame}{Hyperspectral Unmixing}
    Given vectorized spectral signature \(X \in \mathbb{R}^{p \times n}_+\) of an image. \\
    Problem: Identify Endmembers (Grass, Stone,...)

    \begin{center}
    \begin{tabular}{ c c l }
     \(X(:,j) \approx \) &  \(\sum_{k=1}^{r} W(:,k) \) & \(H(k,j)\) \\ 
     spectral signature & spectral signature of k-th endmember & abundance of k-th endmember
    \end{tabular}
    \end{center}
      
\end{frame}

\begin{frame}{Problem v2}
    \begin{block}{Optimization Problem}
            Given \(A \in \mathbb{R}^{m \times n}_+\) non-negative and \emph{rank} \(r \leq \min(m, n)\). \\
            \[ \min_{W \in \mathbb{R}^{m \times r}_+, \ H \in \mathbb{R}^{r \times n}_+ } \quad  \| A - WH \| \]
    \end{block}
    Note that $F(W,H) = \|A - WH\|$ is convex in $U$ and convex in $V$, but not in both!
\end{frame}

\begin{frame}{Stationary Points \& KKT-Conditions}
    % Result of KKT & Stationary
    Checking the KKT-Conditions for \(F(W,H) \) yields the following:
        \[ W \geq 0, \ \nabla_W F = WHH^T - AH^T \geq 0, \ \nabla_W F * W = 0 \]
        \[ H \geq 0, \ \nabla_W F = W^T WH - W^T A \geq 0, \ \nabla_H F * H = 0 \]

    \begin{block}{Stationary Points}
        A pair \((U,V)\) is called a \emph{stationary Point}, if and only if \(U\) and \(V\) satisfy the KKT-Conditions.
    \end{block}
    
\end{frame}

\begin{frame}{Stationary Points \& KKT-Conditions}
    From the KKT-Conditions simple characteristics of the solutions can be derived:
    \begin{block}{Theorem}
        Suppose \( (W,H)\) be a stationary point of the problem, then it holds:
        \[ \frac{1}{2} \|A - WH\|^2 = \frac{1}{2} (\|A\|^2 - \|WH\|^2 )\]
    \end{block}
    This furthermore implies that \( \|A\|^2 \geq \|WH\|^2\), which is only fulfilled at the exact factorization.
\end{frame}

\begin{frame}{Coordinate Descent}
    For $\Omega$ (pointwise) convex \[solve \ \min_{x \in \Omega} f(x) \]
    
    \begin{algorithm}[H]
    \caption{General Coordinate Descent}
        \textbf{Initialization:} \(x \in \mathbb{R}^n\) \\
        \For {$t \gets 1,2, ..., n$} {$solve \ x_i = \underset{\zeta in \Omega_i}{\arg \min} \ f(x_1, ..., x_{i-1}, \zeta, x_{i+1}, ..., x_n)$}
    \end{algorithm}
\end{frame}

\begin{frame}{"Convergence" Theorem}
    \begin{block}{"Convergence" to stationary Points}
        Suppose $f$ is continuously differentiable and furthermore that \\
        \[\forall i \ \forall x: \min_{\zeta \in \Omega_i} f(x_1, ..., x_{i-1}, \zeta, x_{i+1}, ..., x_n)\] 
        is uniquely attained.
        Let \( \{x^k\} \) be the sequence generated by the \emph{Coordinate Descent}, then every limit point is a stationary point.
    \end{block}
\end{frame}

\begin{frame}{Exact Factorization}
    % nonnegative rank
    Now consider the case where $A$ is exactly factorized by $WH^T$.
    \begin{block}{minimal rank}
        The smallest $r$ such that \(\exists W \in \mathbb{R}^{m \times r}_+ \ and \ H \in \mathbb{R}^{r \times n}_+\) such that $A=WH$, is called the \emph{inner rank} and denoted by $rank_{WH}^+ (A)$. 
    \end{block}

\end{frame}

\begin{frame}{Exact Factorization}
    \begin{block}{Lemma}
        \[ rank(A) \leq rank_{WH}^+ (A) \leq \min(m,n)\]
    \end{block}
    
    \[\text{existence of exact factorization of $A$ of rank $r$} \iff \text{determining } rank_{WH}^+ (A) \]
\end{frame}

\begin{frame}{Determining the inner rank}
    One algorithm to determine if $A$ can be factorized with \emph{inner rank} $r$ would be the Renegar algorithm, which scales $(6mn)^{\mathcal{O}(mn)}$. \\
    Since $rank_{WH}^+ (A) \leq \min(m,n)$ this can be done in finite time.
    \begin{block}{Vavasis, 2008}
        \begin{itemize}
            \item exact \emph{NMF} is \emph{NP-hard}
            \item $\exists$ polynomial time local search heuristics
        \end{itemize}
    \end{block}
\end{frame}

% Abdu ------------------------------------------------------------------------

\begin{frame}
    \frametitle{Algorithms for NMF}
    \textbf{The Multiplicative Update Rule}
    \begin{align}
        \min_{W,H>0}f(W,H) = \min_{W,H>0} \frac{1}{2}\sum_{i = 1}^{n}\sum_{j= 1}^{m} (A_{ij} - (WH)_{ij})^{2} \label{op}
    \end{align}
    The most used approach to minimize (1) is a simple multiplicative update method
    proposed by Lee and Seung (2001):\\
    This algorithm is just a special case of the Gradient Descent with a step size 
    \begin{align*}
        \epsilon(W_{ia}^{t}) &= \frac{w^{t}_{ia}}{[W^{t}H^{t}(H^{t})^{T}]_{ia}}\hspace{0.1in} \forall i,a\\
        \epsilon(H_{bj}^{t}) &= \frac{h^{t}_{bj}}{[(W^{t})^{T}W^{t}H^{t}]_{bj}}\forall b,j  
    \end{align*}
\end{frame}
\begin{frame}
    \begin{algorithm}[H]
    \caption{The Multiplicative Update Rule}
     \textbf{Initialization:} $w_{ia}^{1}, h_{bj}^{1} > 0 \hspace{0.1in}\forall i,j,a,b$\;
    \For {$t \gets 1,2..\dots$} {
    $w_{ia}^{t+1} = w_{ia}^{t}\frac{(A(H^{t})^{T})_{ia}}{[W^{t}H^{t}(H^{t})^{T}]_{ia}} \hspace{0.1in}\forall i,a$\;
    $h_{bj}^{t+1} = h_{bj}^{t}\frac{((W^{t+1})^{T}A)_{ia}}{[(W^{t+1})^{T}W^{t+1}H^{t}]_{bj}} \hspace{0.1in}\forall j,b$\;}
    \end{algorithm}  
\end{frame}

\begin{frame}
        This algorithm is a fixed-point type method, meaning that If $[(H^{t})^{T}H^{t}W^{t}]_{ia} \neq 0$ and
        $w_{ia}^{t+1} = w_{ia}^{t}$, then $(A(H^{t})^{T})_{ia} = [W^{t}H^{t}(H^{t})^{T}]_{ia}$, implies 
        $\nabla_{W}f(W^{t}, H^{t})_{ia} = 0$.\\
        Which is part of the KKT condition.
\end{frame}
\begin{frame}
    \frametitle{Proof of convergence of the Multiplicative Update Rule Algorithm}
    \begin{block}{Theorem Lee and Seung, 2001}
        The Euclidean distance $||A - WH||$ is non-increasing under the update rules
        \begin{align*}
            w_{ia} \gets w_{ia}\frac{(AH^{t})_{ia}}{[WH(H)^{T}]_{ia}}, \hspace{0.3in}
            h_{bj} \gets h_{bj}\frac{(W^{T}A)_{ia}}{[(W)^{T}WH]_{bj}} 
        \end{align*}
        The Euclidean distance is invariant under these updates if and only if $W$ and $H$ are at
        a stationary point of the distance.
    \end{block}
\end{frame}
\begin{frame}
    \textbf{Weaknesses and modifications:}\\
    Lee and Seung claim that the limit of the sequence $\{W^{t}, H^{t}\}$ is a 
    stationary point (i.e., a point satisfying the KKT condition). However, Gonzales and Zhang (2005)
    indicate that this claim is wrong as having  the cost function non increasing under the update 
    may not imply the convergence.\\
    Therefore, this multiplicative update method still lacks optimization properties.\\
\end{frame}
\begin{frame}
    we can only make the following statement about
    the convergence of the Lee and Seung multiplicative update algorithms: When the algorithm has converged to a
    limit point, this point is a stationary point.\\\
    
    Also it has been repeatedly shown that the convergence is notoriously slow.
    \end{frame}
    \begin{frame}
    A number of modifications to the original Lee and Seung algorithms have been
    introduced with the objective to overcome these shortcomings.\\
    \end{frame}
    \begin{frame}
    For instance, Lin in 2007 proposed a modification that is guaranteed to converge to a stationary point,
    this algorithm requires more work per iteration than the already slow Lee and Seung algorithm.
\end{frame}
\begin{frame}
    \texrtbf{Fast Multiplicative Update Rule Algorithm by Li-Xin Li, Lin Wu, Hui-Sheng Zhang, and Fang-Xiang Wu (2014)}
        Li-Xin Li, Lin Wu, Hui-Sheng Zhang, and Fang-Xiang Wu Presented a comparison between the three Algorithms
        in there paper in 2014.\\
        Three algorithms are programmed in MATLAB R2013a and run on a computer with the following specifications: 
        a processor of Intel Core 2 Quad CPU Q9450 at 2.66 GHZ 2.67 GHZ and a RAM of 4 GB (3.72 GB usable)
        \textbf{Note:}\\ 
\end{frame}
\begin{frame}
    \includegraphics[height = 3in]{table.jpg}
\end{frame}
\begin{frame}
    \frametitle{Alternating Non-negative Least Squares (ANLS)}
    In this algorithms, a least squares step is followed by another least squares step in an alternating fashion, thus giving rise to the ALS name.\\
    The Alternating Least Squares - ALS algorithms were first introduced by Paatero 1994 , 
    who initially invented the whole NMF theory.
    \begin{algorithm}[H]
        \caption{Basic ALS for NMF}
        \textbf{Initialization:} $W > 0$\;
        \For {t \gets 1, 2\dots}{
            Solve for H the LS equation $W^{T}WH = W^{T}A$\;
            Set all negative elements of in H to 0\;
            Solve for W the LS equation $WHH^{T} = AH^{T}$\;
            Set all negative elements of in W to 0\;
        } 
    \end{algorithm}
\end{frame}
\begin{frame}
    \begin{theorem}
        Any limit point of the sequence $\{W^{t} , H^{t}\}$ generated by ALS Algorithm  is a stationary point of (\ref{op}).
    \end{theorem}
    \end{frame}
    \begin{frame}{Comparison between ANLS and MU}
    \begin{center}
\begin{tabular}{ | m{20em} | m{20em} |} 
  \hline
  \textbf{ANLS}& \textbf{MU}  \\ 
  \hline
  - Can be very fast depending on the implementation & -easy to use  \\ 
  - aids sparsity & \\ 
  \hline
  - once an element in W or H becomes 0, it must remain 0. & - notoriously slow  \\ 
   & - lacks optimization properties\\
   & -the work
 per iteration is high since each iteration requires $O(mnk)$ work.
\end{tabular}
\end{center}   
\end{frame}
\begin{frame}
    \includegraphics[height = 3.0in]{ALS vs MU.jpg}
\end{frame}

\end{document}

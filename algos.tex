\documentclass{beamer}
\usetheme{Frankfurt}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{xpatch}
\usepackage{lipsum}
\usepackage{tcolorbox}
\usepackage{graphicx}
\tcbuselibrary{fitting}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\usepackage{algorithm2e}
\RestyleAlgo{ruled}
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage{pseudocode}
% \usepackage{float}
\makeatletter
\patchcmd\beamer@@tmpl@frametitle{\insertframetitle}{\insertsection\space-- \insertframetitle}{}{}
\makeatother
\begin{document}
\begin{spacing}{1.0}
\begin{frame}
\frametitle{Algorithms for NMF}
\textbf{The Multiplicative Update Rule}
\begin{align}
    \min_{W,H>0}f(W,H) = \min_{W,H>0} \frac{1}{2}\sum_{i = 1}^{n}\sum_{j= 1}^{m} (A_{ij} - (WH)_{ij})^{2} \label{op}
\end{align}
The most used approach to minimize (1) is a simple multiplicative update method
proposed by Lee and Seung (2001):\\
This algorithm is just a special case of the Gradient Descent with a step size 
\begin{align*}
    \epsilon(W_{ia}^{t}) &= \frac{w^{t}_{ia}}{[W^{t}H^{t}(H^{t})^{T}]_{ia}}\hspace{0.1in} \forall i,a\\
    \epsilon(H_{bj}^{t}) &= \frac{h^{t}_{bj}}{[(W^{t})^{T}W^{t}H^{t}]_{bj}}\forall b,j  
\end{align*}
\end{frame}
\begin{frame}
\begin{algorithm}[H]
    \caption{The Multiplicative Update Rule}
     \textbf{Initialization:} $w_{ia}^{1}, h_{bj}^{1} > 0 \hspace{0.1in}\forall i,j,a,b$\;
    \For {$t \gets 1,2..\dots$} {
    $w_{ia}^{t+1} = w_{ia}^{t}\frac{(A(H^{t})^{T})_{ia}}{[W^{t}H^{t}(H^{t})^{T}]_{ia}} \hspace{0.1in}\forall i,a$\;
    $h_{bj}^{t+1} = h_{bj}^{t}\frac{((W^{t+1})^{T}A)_{ia}}{[(W^{t+1})^{T}W^{t+1}H^{t}]_{bj}} \hspace{0.1in}\forall j,b$\;}
    \end{algorithm}
\end{frame}
\begin{frame}
        This algorithm is a fixed-point type method, meaning that If $[(H^{t})^{T}H^{t}W^{t}]_{ia} \neq 0$ and
        $w_{ia}^{t+1} = w_{ia}^{t}$, then $(A(H^{t})^{T})_{ia} = [W^{t}H^{t}(H^{t})^{T}]_{ia}$, implies 
        $\nabla_{W}f(W^{t}, H^{t})_{ia} = 0$.\\
        Which is part of the KKT condition.
\end{frame}
\begin{frame}
    \frametitle{Proof of convergence of the Multiplicative Update Rule Algorithm}
    \begin{theorem}[Lee and Seung, 2001]
        The Euclidean distance $||A - WH||$ is non-increasing under the update rules
        \begin{align*}
            w_{ia} \gets w_{ia}\frac{(AH^{t})_{ia}}{[WH(H)^{T}]_{ia}}, \hspace{0.3in}
            h_{bj} \gets h_{bj}\frac{(W^{T}A)_{ia}}{[(W)^{T}WH]_{bj}} 
        \end{align*}
        The Euclidean distance is invariant under these updates if and only if $W$ and $H$ are at
        a stationary point of the distance.
    \end{theorem}
\end{frame}
\begin{frame}
    \frametitle{Proof}
    \begin{definition}[Auxilary Function]
        A function $G(h, k)$ is called an auxiliary function for $f(h) $if:\\
        $1. G(h,h) = f(h)$\\
        $2. G(h,k) \geq f(h)$ for all $k$. 
    \end{definition}
    The following lemma illustrates why the concept of auxiliary function can be useful to
    minimize $F(h)$ and to find a local minimum.
    \begin{lemma}[Iterative minimization, Lee and Seung, 2001)]
        If $G$ is an auxiliary function, then $F$ is nonincreasing under the update\\
        \begin{align}
            F(h^{t+1}) = \underset{h}{\mathrm{argmin}}G(h, h^{t})
        \end{align}
    \end{lemma}
\end{frame}
\begin{frame}
    \textbf{Proof}:\\
    $F(h^{t+1}) \leq G(h^{t+1}, h^{t}) \leq G(h^{t}, h^{t}) = F(h^{t})$\\
    Let us now construct an Auxilary Function for our Loss function
    \begin{lemma}
        If $K(h^{t})$ is the diagonal matrix
        \begin{align*}
            K_{ij} = \delta_{ij}\frac{(W^{T}Wh^{t})_{i}}{h_{i}^{t}}
        \end{align*}
        then 
        \begin{align}
            G(h, h^{t}) = F(h^{t}) + (h + h^{t})^{T}\nabla F(h^{t}) + \frac{1}{2}
            (h - h^{t})^{T}K(h^{t})(h - h^{t})
        \end{align}
        is an auxilary function for 
        \begin{align}
            F(h) = \frac{1}{2}\sum_{k}(m_{k} - (Wh)_{k})
        \end{align}
    \end{lemma}
\end{frame}
\begin{frame}
    \textbf{Proof:}\\
    $G(h, h) = F(h)$, follows from the difinition of $G$, now we need $G(h,k) \leq F(h) \hspace{0.1in} \forall k$\\
    see \href{https://proceedings.neurips.cc/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf}
    {Here for the proof}.\\
    With all this preliminary work the proof of our Theorem  can be demonstrated.
\end{frame}
\begin{frame}
        \textbf{Proof:}\\
        Using the auxiliary function, which was defined in the previous Lemma and solving (2)
        by setting the gradient to zero leads to the update rule:
        \begin{align*}
            h^{t+1} = h^{t} - K(h^{t})^{-1}\nabla F(h^{t})
        \end{align*}
        According to the Iterative minimization Lemma, F is non-increasing under this update
        rule. It can be obtained
        \begin{align*}
            h^{t+1}_{i} = h^{t}_{i}\frac{(W^{T}m)_{i}}{(W^{t}Wh^{t})_{i}}
        \end{align*}
        by applying the explicit structure of $K(h^{t})^{-1}$ and $\nabla F(h^{t})$. Rewritten
        in matrix form this is equivalent to the update rule.
\end{frame}
\begin{frame}
    \textbf{Weaknesses and modifications:}\\
    Lee and Seung claim that the limit of the sequence $\{W^{t}, H^{t}\}$ is a 
    stationary point (i.e., a point satisfying the KKT condition). However, Gonzales and Zhang (2005)
    indicate that this claim is wrong as having  the cost function non increasing under the update 
    may not imply the convergence.\\
    Therefore, this multiplicative update method still lacks optimization properties.\\
    It has been repeatedly shown that the convergence is notoriously slow.
    
    A number of modifications to the original Lee and Seung algorithms have been
    introduced with the objective to overcome these shortcomings.\\

    For instance, Lin in 2007 proposed a modification that is guaranteed to converge to a stationary point,
    this algorithm requires more work per iteration than the already slow Lee and Seung algorithm.
\end{frame}
\begin{frame}
    \begin{tcolorbox}[fit,width=\textwidth,height=.85\textheight,size=minimal,colback=white,fit algorithm=fontsize,colframe=white]
\frametitle{Fast Multiplicative Update Rule Algorithm by Li-Xin Li, Lin Wu, Hui-Sheng Zhang, and Fang-Xiang Wu (2014)}
\begin{algorithm}[H]
    \caption{Fast Multiplicative Update Rule Algorithm}
        \textbf{Initialize:} $w_{ia}^{1}, h_{bj}^{1} > 0 \hspace{0.1in}\forall i,j,a,b$\;
        \For{$t \gets 1,2..\dots$} {
        \eIf {w_{b}^{t} = 0}{ $h_{bj}^{t+1} = 0$, for all $j$\;
        Where\; 
        $w_{b}^{t}$ is the b-th column vector of $W^{t}$\;}
        {$h_{bj}^{t+1} = max(0, \bar{h}_{bj}^{t+1})$\;
         Where \;
         $\bar{h}_{bj}^{t+1} = h_{bj}^{t} - \frac{1}{||w_{b}^{t}||^{2}}\frac{\partial f(W^{t},H^{t})}{\partial h_{bj}}$\;
        }
        \eIf {h_{a}^{t} = 0}{ $w_{ia}^{t+1} = 0$, for all $i$\;
        Where\; 
        $h_{a}^{t}$ is the b-th column vector of $H^{t}$\;}
        {$w_{ia}^{t+1} = max(0, \bar{w}_{ia}^{t+1})$\;
         Where \;
         $\bar{w}_{ia}^{t+1} = w_{ia}^{t} - \frac{1}{||h_{a}^{t}||^{2}}\frac{\partial f(W^{t},H^{t})}{\partial w_{ia}}$\;
        }
        }
\end{algorithm}
\end{tcolorbox}
\end{frame}
\begin{frame}
    \begin{theorem}
        The presented Algorithm is (in most cases, strictly) faster than the Multiplicative Update Rule
        Algorithm.
    \end{theorem}
\end{frame}
\begin{frame}
        Li-Xin Li, Lin Wu, Hui-Sheng Zhang, and Fang-Xiang Wu Presented a comparison between the three Algorithms
        in there paper in 2014.\\
        Three algorithms areprogrammed in MATLAB R2013a and run on a computer with the following specifications: 
        a processor of Intel Core 2 Quad CPU Q9450 at 2.66 GHZ 2.67 GHZ and a RAM of 4 GB (3.72 GB usable)
        \textbf{Note:}\\ 
        - Algo 2 = Multiplicative Update Rule\\
        - Algo 3 = modified Multiplicative Update Rule\\
        - Algo 4 = Algorithm 2
\end{frame}
\begin{frame}
    \includegraphics[height = 3in]{/Users/abdelbastnassiri/Desktop/data 1.png}
\end{frame}
\begin{frame}
    \includegraphics[height = 3in]{/Users/abdelbastnassiri/Desktop/data 2.png}
\end{frame}
\begin{frame}
    \includegraphics[height = 3in]{/Users/abdelbastnassiri/Desktop/data 3.png}
\end{frame}
\begin{frame}
    \frametitle{Alternating Non-negative Least Squares (ANLS)}
    The Alternating Least Squares - ALS algorithms were first introduced by Paatero 1994 , 
    who initially invented the whole NMF theory.
    \begin{algorithm}[H]
        \caption{Basic ALS for NMF}
        \textbf{Initialization:} $W > 0$\;
        \For {t \gets 1, 2\dots}{
            Solve for H the LS equation $W^{T}WH = W^{T}A$\;
            Set all negative elements of in H to 0\;
            Solve for W the LS equation $WHH^{T} = AH^{T}$\;
            Set all negative elements of in W to 0\;
        } 
    \end{algorithm}
\end{frame}
\begin{frame}
    \begin{theorem}
        Any limit point of the sequence $\{W^{t} , H^{t}\}$ generated by ALS Algorithm  is a stationary point of (\ref{op}).
    \end{theorem}
    the advantages of the ALS is that it is fast, aids sparcity and with good implementation it can be faster 
    than SVD    
\end{frame}
\end{spacing}
\end{document}